{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca7ea51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TfidfVectorizer3' from 'sklearn.feature_extraction.text' (/Users/marcuschandra/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp_store_scraper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AppStore\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NMF\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer3\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatentDirichletAllocation\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TfidfVectorizer3' from 'sklearn.feature_extraction.text' (/Users/marcuschandra/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from app_store_scraper import AppStore\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer3\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fde48e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1912247331.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install app_store_scraper\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install app_store_scraper\n",
    "pip install google-play-scraper\n",
    "pip install --upgrade pip\n",
    "pip install pandas spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295fd26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m custom_stop_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommend\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelpful\u001b[39m\u001b[38;5;124m'\u001b[39m,]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:449\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = ['recommend', 'love', 'helpful',]\n",
    "stop_words.update(custom_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "italki = AppStore(country = 'us', app_name = 'italki-language-learning', app_id = '1140000003')\n",
    "italki.review(how_many = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2619e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'italki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m italkidf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mitalki\u001b[49m\u001b[38;5;241m.\u001b[39mreviews), columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m italkidf2 \u001b[38;5;241m=\u001b[39m italkidf\u001b[38;5;241m.\u001b[39mjoin(pd\u001b[38;5;241m.\u001b[39mDataFrame(italkidf\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m      3\u001b[0m italkidf2\n",
      "\u001b[0;31mNameError\u001b[0m: name 'italki' is not defined"
     ]
    }
   ],
   "source": [
    "italkidf = pd.DataFrame(np.array(italki.reviews), columns = ['review'])\n",
    "italkidf2 = italkidf.join(pd.DataFrame(italkidf.pop('review').tolist()))\n",
    "italkidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ec70a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 128)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italki_positive = italkidf2[italkidf2['rating']>=4]\n",
    "italki_negative = italkidf2[italkidf2['rating']<=2]\n",
    "len(italki_positive), len(italki_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0086b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews_app_store = italki_positive['review'].tolist()\n",
    "negative_reviews_app_store = italki_negative['review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bc1ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_play_scraper import app\n",
    "from google_play_scraper import Sort, reviews_all\n",
    "\n",
    "gp_italki = reviews_all(\n",
    "    'com.italki.app',\n",
    "    sleep_milliseconds=0, # defaults to 0\n",
    "    lang='en', # defaults to 'en'\n",
    "    country='us', # defaults to 'us'\n",
    "    sort=Sort.NEWEST, # defaults to Sort.MOST_RELEVANT\n",
    ") \n",
    "\n",
    "italki_gp_df = pd.DataFrame(np.array(gp_italki),columns=['review'])\n",
    "italki_gp_df = italki_gp_df.join(pd.DataFrame(italki_gp_df.pop('review').tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bf2b6bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1124, 706)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italki_gp_positive = italki_gp_df[italki_gp_df['score']>=4]\n",
    "italki_gp_negative = italki_gp_df[italki_gp_df['score']<=2]\n",
    "len(italki_gp_positive), len(italki_gp_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "448deb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews_gp = italki_gp_positive['content'].tolist()\n",
    "negative_reviews_gp = italki_gp_negative['content'].tolist()\n",
    "total_positive = positive_reviews_gp + positive_reviews_app_store\n",
    "total_negative = negative_reviews_gp + negative_reviews_app_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35523100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marcuschandra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcuschandra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e7ca7633",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app: 646\n",
      "italki: 407\n",
      "language: 379\n",
      "great: 283\n",
      "teachers: 268\n",
      "good: 244\n",
      "learning: 240\n",
      "teacher: 222\n",
      "learn: 221\n",
      "really: 170\n",
      "lessons: 147\n",
      "use: 139\n",
      "easy: 138\n",
      "love: 130\n",
      "like: 124\n",
      "one: 121\n",
      "languages: 109\n",
      "find: 108\n",
      "best: 105\n",
      "platform: 101\n",
      "way: 101\n",
      "lesson: 99\n",
      "amazing: 91\n",
      "time: 88\n",
      "recommend: 87\n",
      "new: 82\n",
      "native: 80\n",
      "get: 77\n",
      "using: 73\n",
      "well: 69\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample data\n",
    "filtered_reviews = []\n",
    "for review in total_positive:\n",
    "    word_tokens = word_tokenize(review.lower())  # Tokenize and convert to lowercase\n",
    "    filtered_review = [word for word in word_tokens if not word in stop_words and word.isalpha()]\n",
    "    filtered_reviews.extend(filtered_review)\n",
    "\n",
    "    \n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(filtered_reviews)\n",
    "for word, frequency in fdist.most_common(30):  # Adjust the number to see more or fewer common words\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "de33a5b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "email way get nothing without support still tried send using\n",
      "Topic #2:\n",
      "phone much many using make people one also great credits\n",
      "Topic #3:\n",
      "website never cannot send give like get teachers also could\n",
      "Topic #4:\n",
      "problem one get bad times help back good tried trying\n",
      "Topic #5:\n",
      "see lessons also account want phone problem back still give\n",
      "Topic #6:\n",
      "like able message would used use app way something please give\n",
      "Topic #7:\n",
      "teachers support times available many still useless great help always\n",
      "Topic #8:\n",
      "lesson bad could make every available see update new show\n",
      "Topic #9:\n",
      "class show still option another give would every cannot fix\n",
      "Topic #10:\n",
      "free good messages lessons also first still using please would\n",
      "Topic #11:\n",
      "want people learn good like learning trying help great could\n",
      "Topic #12:\n",
      "service customer really useless support great go bad credits used\n",
      "Topic #13:\n",
      "update need working something always account one like know still\n",
      "Topic #14:\n",
      "error pay try every message application please working problem cannot\n",
      "Topic #15:\n",
      "sign learning know user way find able could give cannot\n",
      "Topic #16:\n",
      "new please fix trying try always back application using without\n",
      "Topic #17:\n",
      "work get lesson pay without first help money available service\n",
      "Topic #18:\n",
      "tried nothing application first try could give use app back using\n",
      "Topic #19:\n",
      "money account credits back without lessons tried available give another\n",
      "Topic #20:\n",
      "find lessons pay go way lesson teachers never make option\n"
     ]
    }
   ],
   "source": [
    "#LDA model\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3), stop_words=stop_words, max_df=0.1, min_df=25, max_features=1000)\n",
    "review_term_matrix = vectorizer.fit_transform(total_negative)\n",
    "\n",
    "n_topics = 20\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "lda.fit(review_term_matrix)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{topic_idx+1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4f62d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcuschandra/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "like would far know please lesson book time also found\n",
      "Topic #2:\n",
      "easy use classes way schedule affordable teaching book community application\n",
      "Topic #3:\n",
      "amazing much application skills tutor highly people could practice community\n",
      "Topic #4:\n",
      "nice people always time know could able tutor use lesson\n",
      "Topic #5:\n",
      "best way english teaching different spanish native know speakers improve\n",
      "Topic #6:\n",
      "excellent lot application spanish thanks far highly affordable way many\n",
      "Topic #7:\n",
      "languages different many lot world people also choose way want\n",
      "Topic #8:\n",
      "find tutor way needs try also different easy highly students\n",
      "Topic #9:\n",
      "one classes lot try get make work could people also\n",
      "Topic #10:\n",
      "lessons lesson much lot thanks also get book always thank\n",
      "Topic #11:\n",
      "platform using years students needs awesome teaching thank highly classes\n",
      "Topic #12:\n",
      "useful english application thank much thanks improve think help lesson\n",
      "Topic #13:\n",
      "works well lesson work thank better students need using spanish\n",
      "Topic #14:\n",
      "new way work people awesome thanks teaching would classes tutors\n",
      "Topic #15:\n",
      "native experience time help speaking tutors found many speakers get\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcuschandra/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#NMF model\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.1, min_df=30, stop_words=stop_words)\n",
    "review_term_matrix = tfidf_vectorizer.fit_transform(total_positive)\n",
    "nmf_model = NMF(n_components=15, random_state=1)  # n_components is the number of topics\n",
    "nmf_topic_matrix = nmf_model.fit_transform(review_term_matrix)\n",
    "\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07917c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
